---
title: "Explore geoarrow"
---

## Write parquet locally

-   [geoarrow-r](https://geoarrow.org/geoarrow-r/)

```{r}
#| label: write-parquet

# packages
librarian::shelf(
  arrow, dplyr, geoarrow, here, mapview, sf, tibble,
  quiet = T)

# paths
nc_gp <- system.file("gpkg/nc.gpkg", package = "sf")
nc_pq <- here("data/geoarrow/nc.parquet")

# read geopackage
nc <- read_sf(nc_gp)

# show North Carolina counties
mapView(nc)

# write parquet
if (!file.exists(nc_pq))
  nc |> 
    as_tibble() |> 
    write_parquet(nc_pq)

# compare file sizes
tibble(
  obj = c("nc_gp", "nc_pq")) |>
  mutate(
    mb = file.info(c(nc_gp, nc_pq))$size/1024^2)
```

## Read parquet locally

```{r}
#| label: read-local

# filter and map
open_dataset(nc_pq) |> 
  filter(startsWith(NAME, "A")) |>
  select(NAME, geom) |> 
  st_as_sf() |> 
  mapView(layer.name = "A counties")
```

## Push parquet to AWS S3 Bucket

-   [README](https://cran.r-project.org/web/packages/aws.s3/readme/README.html)
-   [Using Amazon S3 with R – Notes from a data witch](https://blog.djnavarro.net/posts/2022-03-17_using-aws-s3-in-r/#accounts-and-credentials)

<https://us-east-1.console.aws.amazon.com/s3/buckets/oceanmetrics.io-public>

Bucket policy for public read access:

``` json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject"
            ],
            "Resource": [
                "arn:aws:s3:::Bucket-Name/*"
            ]
        }
    ]
}
```

Policy for Cross-origin resource sharing (CORS):

- [Elements of a CORS configuration - Amazon Simple Storage Service](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ManageCorsUsing.html)

```json

[
    {
        "AllowedHeaders": [],
        "AllowedMethods": [
            "GET"
        ],
        "AllowedOrigins": [
            "*"
        ],
        "ExposeHeaders": []
    }
]
```

```{r}
#| label: push-aws

stopifnot(arrow_with_s3())

librarian::shelf(
  glue, janitor, readr, tibble,
  quiet = T)
options(readr.show_col_types = F)

d_aws <- read_csv(
  '/Users/bbest/Library/CloudStorage/GoogleDrive-ben@ecoquants.com/My Drive/private/ben_ben@ecoquants.com_console.aws.amazon.com_accessKeys.csv') |> 
  clean_names()
Sys.setenv(
  "AWS_ACCESS_KEY_ID"     = d_aws$access_key_id,
  "AWS_SECRET_ACCESS_KEY" = d_aws$secret_access_key,
  "AWS_DEFAULT_REGION"    = "us-east-1")
bucket <- "oceanmetrics.io-public"

# load/reload aws.s3 to read credentials
unloadNamespace("aws.s3")
librarian::shelf(
  aws.s3,
  quiet = T)
# bucketlist()

stopifnot(bucket_exists(bucket))
stopifnot(exists("nc_pq"))

o      <- basename(nc_pq)
o_file <- nc_pq

if (!object_exists(o, bucket))
  put_object(o_file, o, bucket)

# put_acl(o, bucket, acl = "public-read")
# "The bucket does not allow ACLs"

# check
o_s3 <- glue("s3://{bucket}/{o}")
object_exists(o_s3)
```

## Read parquet from S3

```{r}
#| label: read-s3

# standalone R chunk

librarian::shelf(
  arrow, dplyr, geoarrow, glue, mapview, sf,
  quiet = T)

bucket <- "oceanmetrics.io-public"
obj    <- "nc.parquet"

open_dataset(glue("s3://{bucket}/{obj}")) |> 
  filter(startsWith(NAME, "A")) |>
  select(NAME, geom) |> 
  st_as_sf() |> 
  mapView(layer.name = "A counties")
```

## Create GeoJSON from parquet for easier JavaScript consumption

```{r}
#| label: convert-to-geojson
#| eval: true

librarian::shelf(
  arrow, dplyr, geoarrow, glue, jsonlite, mapview, sf,
  quiet = T)

bucket <- "oceanmetrics.io-public"
obj    <- "nc.parquet"

# Convert the "A" counties to GeoJSON and save to a file
if (!file.exists(here("data/geoarrow/nc_a_counties.geojson"))) {
  # Read the parquet file from S3
  nc_a <- open_dataset(glue("s3://{bucket}/{obj}")) |>
    filter(startsWith(NAME, "A")) |>
    select(NAME, geom) |>
    st_as_sf()

  # Write as GeoJSON
  st_write(
    nc_a,
    here("data/geoarrow/nc_a_counties.geojson"),
    driver = "GeoJSON")

  # Also push to S3
  if (require(aws.s3) && object_exists(bucket)) {
    put_object(
      here("data/geoarrow/nc_a_counties.geojson"),
      "nc_a_counties.geojson",
      bucket)
  }
}

# Show the conversion output
file.info(here("data/geoarrow/nc_a_counties.geojson"))$size/1024
```

## Read parquet from S3 using Observable JavaScript

```{=html}
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.css">
```

```{ojs}
//| label: read-s3-js
//| code-fold: true

// We're now using OJS require() which handles dependency loading,
// but we'll keep a small delay to ensure UI elements are ready
function waitForScripts() {
  return new Promise(resolve => setTimeout(resolve, 300))
}

// Define the sources
bucket = "oceanmetrics.io-public"
obj = "nc.parquet"
geojsonObj = "nc_a_counties.geojson"
// url = "https://" + bucket + ".s3.amazonaws.com/" + obj
url = "https://s3.us-east-1.amazonaws.com/" + bucket + "/" + obj
geojsonUrl = "https://s3.us-east-1.amazonaws.com/" + bucket + "/" + geojsonObj

// Create a map container
viewof map = {
  const container = document.createElement("div")
  container.style.height = "500px"
  return container
}

// Use OJS Observable require feature to check if dependencies are available
checkDependencies = {
  return html`<div>Using Observable JS require() for dependency management</div>`
}

// Create the map
mapData = {
  // Create a promise-based solution for better compatibility
  return new Promise(async (resolve) => {
    // Wait for scripts to load first
    await waitForScripts()
    try {
      // Import Leaflet
      const L = await require("leaflet@1.9.4")

      // Create Leaflet map
      const map = L.map(viewof map).setView([35.6, -79.4], 7)

      // Add base map
      L.tileLayer("https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png", {
        attribution: '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
      }).addTo(map)

      // Load parquet file from S3
      const response = await fetch(url)
      const buffer = await response.arrayBuffer()

      // Directly import arrow using require (with specific functions)
      const arrow = await require("apache-arrow@14.0.2")
      console.log("Arrow loaded via require:", arrow)

      // Make sure tableFromIPC is available
      if (!arrow.tableFromIPC) {
        console.error("tableFromIPC not found, importing directly")
        const { tableFromIPC } = await require("apache-arrow@14.0.2/Arrow")
        arrow.tableFromIPC = tableFromIPC
      }

      // Attempt to parse Parquet using parquet-wasm
      let table
      try {
        console.log("Loading parquet-wasm...")
        const parquetWasm = await require("parquet-wasm@0.4.2")

        // Initialize wasm and parse parquet file
        console.log("Initializing parquet-wasm...")
        await parquetWasm.default()
        const parquetData = new Uint8Array(buffer)
        console.log("Parsing parquet data...")
        const arrowTable = parquetWasm.readParquet(parquetData)

        // Convert to Arrow table
        console.log("Converting to Arrow table...")
        table = arrow.tableFromIPC(arrowTable.intoIPCStream())
      } catch (parquetError) {
        console.error("Error parsing with parquet-wasm:", parquetError)

        // Fallback: Try using arrow-js directly (if it supports Parquet)
        try {
          console.log("Trying alternative parquet parsing methods...")

          if (arrow.tableFromParquet) {
            table = await arrow.tableFromParquet(new Uint8Array(buffer))
          } else if (arrow.readParquet) {
            table = await arrow.readParquet(new Uint8Array(buffer))
          } else {
            throw new Error("No suitable method found to parse Parquet files")
          }
        } catch (fallbackError) {
          console.error("Fallback parsing also failed:", fallbackError)
          // Final fallback: Try loading the pre-converted GeoJSON
          console.log("Attempting to load pre-converted GeoJSON instead...")
          return fetch(geojsonUrl)
            .then(response => {
              if (!response.ok) {
                throw new Error(`Failed to fetch GeoJSON: ${response.status} ${response.statusText}`)
              }
              return response.json()
            })
            .then(geojsonData => {
              // We'll handle this GeoJSON directly later
              // Skip all parquet parsing and table operations
              console.log("Successfully loaded GeoJSON data", geojsonData)
              throw {
                name: "UseGeoJSONInstead",
                message: "Using GeoJSON instead of Parquet",
                geojsonData
              }
            })
        }
      }

      // Get column names for debugging
      console.log("Columns:", table.schema.fields.map(f => f.name))

      // Filter for counties starting with 'A'
      const nameColumn = table.getColumn('NAME')
      const indices = []

      // Manually filter counties starting with 'A'
      for (let i = 0; i < nameColumn.length; i++) {
        const name = nameColumn.get(i).toString()
        if (name.startsWith('A')) {
          indices.push(i)
        }
      }

      console.log(`Found ${indices.length} counties starting with 'A'`)

      // Import d3 directly
      const d3 = await require("d3@7.8.5")

      // Create filtered table
      const filtered = table.select(table.schema.fields.map(f => f.name))
                           .filter(d3.range(table.numRows).map(i => indices.includes(i)))

      // Convert to GeoJSON using geoarrow
      let geoJSON
      try {
        // Import geoarrow directly
        const geoarrow = await require("geoarrow-js@0.2.1")

        // Use the imported geoarrow module
        geoJSON = geoarrow.tableToGeoJSON(filtered)
      } catch (error) {
        console.error("Error converting to GeoJSON:", error)

        // Fallback: Manual GeoJSON creation for demonstration
        // This is just a placeholder showing counties as simple polygons
        geoJSON = {
          type: "FeatureCollection",
          features: indices.map(i => ({
            type: "Feature",
            properties: { NAME: nameColumn.get(i).toString() },
            geometry: {
              type: "Polygon",
              coordinates: [
                [[-79.5, 35.5], [-79.5, 36], [-79, 36], [-79, 35.5], [-79.5, 35.5]]
              ]
            }
          }))
        }
      }

      // Add GeoJSON to map
      L.geoJSON(geoJSON, {
        style: {
          color: "#ff7800",
          weight: 2,
          opacity: 0.65
        },
        onEachFeature: (feature, layer) => {
          if (feature.properties && feature.properties.NAME) {
            layer.bindPopup(`<b>${feature.properties.NAME}</b>`)
          }
        }
      }).addTo(map)

      resolve("Map loaded successfully with counties starting with 'A'")
    } catch (error) {
      console.error("Error loading data:", error)

      // Check if this is our special GeoJSON case
      if (error && error.name === "UseGeoJSONInstead" && error.geojsonData) {
        console.log("Using pre-converted GeoJSON data instead")

        // Add the GeoJSON directly to the map
        L.geoJSON(error.geojsonData, {
          style: {
            color: "#ff7800",
            weight: 2,
            opacity: 0.65
          },
          onEachFeature: (feature, layer) => {
            if (feature.properties && feature.properties.NAME) {
              layer.bindPopup(`<b>${feature.properties.NAME}</b>`)
            }
          }
        }).addTo(map)

        resolve("Map loaded successfully with counties starting with 'A' (using GeoJSON)")
        return
      }

      // Add error message to map
      const errorDiv = document.createElement('div')
      errorDiv.innerHTML = `<p style="color: red;">Error loading data: ${error.message}</p>`
      viewof map.appendChild(errorDiv)

      resolve("Error: " + error.message)
    }
  })
}
```

## OLD: Push to Google Cloud Storage \[read not working\]

-   [11. Cloud storage – The {targets} R package user manual](https://books.ropensci.org/targets/cloud-storage.html#:~:text=Install%20the%20googleCloudStorageR%20R%20package,or%20googleCloudStorageR%3A%3Agcs_create_bucket()%20.)
-   [googleCloudStorageR • googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html)
-   [Integrate Google Cloud Storage and rgee](https://cran.r-project.org/web/packages/rgee/vignettes/rgee05.html): use "fine-grained"

```{r}
#| label: push-gcs
#| eval: false

# set env BEFORE loading googleCloudStorageR
gcs_json   = "/Users/bbest/Library/CloudStorage/GoogleDrive-ben@ecoquants.com/My Drive/private/offhab-google-service-account_09e7228ac965.json"
gcs_bucket = "oceanmetrics"
Sys.setenv(
  "GCS_DEFAULT_BUCKET" = gcs_bucket,
  "GCS_AUTH_FILE"      = gcs_json)

# packages
librarian::shelf(
  googleCloudStorageR, targets,
  quiet = T)

gcs_get_bucket(gcs_bucket)

o_file <- nc_pq
o      <- basename(nc_pq)

# upload
gcs_upload(
  file            = o_file,
  name            = o,
  object_metadata = gcs_metadata_object(
    o,
    metadata = list(
      state = "NC",
      notes = "test metadata")))

# make publicly visible
gcs_update_object_acl(
  o, entity_type = "allUsers", role = "READER")

# get URL
(url <- gcs_download_url(o))
```

## OLD: Reinstall arrow with GCS support \[not working\]

-   [Using cloud storage (S3, GCS) • Arrow R Package](https://arrow.apache.org/docs/r/articles/fs.html)

```{r}
#| label: reinstall-arrow
#| eval: false

# Sys.setenv(ARROW_R_DEV=TRUE); install.packages("arrow")
librarian::shelf("arrow",
  quiet = T)

# https://github.com/apache/arrow/issues/44859#issuecomment-2502344866
# brew install openssl@3
Sys.setenv("ARROW_R_DEV"=TRUE, "NOT_CRAN" = "true")
install.packages("arrow", type = "source")
install.packages("arrow")

arrow_with_gcs()
arrow_info()

nc_pq <- "https://storage.cloud.google.com/oceanmetrics/nc.parquet"

open_dataset(nc_pq) |> 
  filter(startsWith(NAME, "A")) |>
  select(NAME, geom) |> 
  st_as_sf() |> 
  mapView(layer.name = "A counties")
```
